{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
      "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
      "INFO:oauth2client.client:Timeout attempting to reach GCE metadata service.\n",
      "WARNING:apache_beam.internal.gcp.auth:Unable to find default credentials to use: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.\n",
      "Connecting anonymously.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', 'C:\\\\Users\\\\alexa\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-8b8b9196-a684-4b19-ac81-a7fa4b9e86e1.json']\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x000001A8D1AA7F70> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x000001A8D1AA80D0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x000001A8D1AA83A0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x000001A8D1AA8550> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x000001A8D1AA85E0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x000001A8D1AA8700> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x000001A8D1AA8790> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x000001A8D1AA8820> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x000001A8D1AA88B0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x000001A8D1AA8AF0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x000001A8D1AA8A60> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x000001A8D1AA8B80> ====================\n",
      "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x000001A8D1C66EE0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read/Read/Impulse_4)+(ref_AppliedPTransform_Read/Read/Map(<lambda at iobase.py:899>)_5))+(Read/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_2_split/Write)\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((ref_PCollection_PCollection_2_split/Read)+(Read/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Split_8))+(ref_AppliedPTransform_PairWIthOne_9))+(GroupAndSum/Precombine))+(GroupAndSum/Group/Write)\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((GroupAndSum/Group/Read)+(GroupAndSum/Merge))+(GroupAndSum/ExtractOutputs))+(ref_AppliedPTransform_Format_14)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Create the Apache-beam pipeline used in Google Cloud's Dataflows\n",
    "    to read the NERR csv files stored in Google Storage and upload\n",
    "    them into Cloud SQL\n",
    "\"\"\"\n",
    "\n",
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"A word-counting workflow.\"\"\"\n",
    "\n",
    "# pytype: skip-file\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "\n",
    "from past.builtins import unicode\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "\n",
    "class WordExtractingDoFn(beam.DoFn):\n",
    "  \"\"\"Parse each line of input text into words.\"\"\"\n",
    "  def process(self, element):\n",
    "    \"\"\"Returns an iterator over the words of this element.\n",
    "\n",
    "    The element is a line of text.  If the line is blank, note that, too.\n",
    "\n",
    "    Args:\n",
    "      element: the element being processed\n",
    "\n",
    "    Returns:\n",
    "      The processed element.\n",
    "    \"\"\"\n",
    "    return re.findall(r'[\\w\\']+', element, re.UNICODE)\n",
    "\n",
    "\n",
    "def run(argv=None, save_main_session=True):\n",
    "  \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--input',\n",
    "      dest='input',\n",
    "      default='gs://dataflow-samples/shakespeare/kinglear.txt',\n",
    "      help='Input file to process.')\n",
    "  parser.add_argument(\n",
    "      '--output',\n",
    "      dest='output',\n",
    "      required=False,\n",
    "      help='Output file to write results to.')\n",
    "  known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "  # We use the save_main_session option because one or more DoFn's in this\n",
    "  # workflow rely on global context (e.g., a module imported at module level).\n",
    "  pipeline_options = PipelineOptions(pipeline_args)\n",
    "  pipeline_options.view_as(SetupOptions).save_main_session = save_main_session\n",
    "\n",
    "  # The pipeline will be run on exiting the with block.\n",
    "  with beam.Pipeline(options=pipeline_options) as p:\n",
    "\n",
    "    # Read the text file[pattern] into a PCollection.\n",
    "    lines = p | 'Read' >> ReadFromText(known_args.input)\n",
    "\n",
    "    counts = (\n",
    "        lines\n",
    "        | 'Split' >>\n",
    "        (beam.ParDo(WordExtractingDoFn()).with_output_types(unicode))\n",
    "        | 'PairWIthOne' >> beam.Map(lambda x: (x, 1))\n",
    "        | 'GroupAndSum' >> beam.CombinePerKey(sum))\n",
    "\n",
    "    # Format the counts into a PCollection of strings.\n",
    "    def format_result(word, count):\n",
    "      return '%s: %d' % (word, count)\n",
    "\n",
    "    output = counts | 'Format' >> beam.MapTuple(format_result)\n",
    "\n",
    "    # Write the output using a \"Write\" transform that has side effects.\n",
    "    # pylint: disable=expression-not-assigned\n",
    "    # output | 'Write' >> WriteToText(known_args.output)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "  run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_v1",
   "language": "python",
   "name": "machine_learning_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
